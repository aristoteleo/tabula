Model:
  # tokenizer
  padding_id: 60694     # the <pad> id in vocab
  padding_value: 0.0    # the value at the position <pad>
  # encoder
  in_feature: 1200            # the fixed length of the gene values sequence
  embedding_in_feature: 60697 # the size of vocab
  d_token: 192                # the hidden dimension of encoding
  # transformer
  contrastive_out_feature: 128      # the feature dimension of contrastive loss
  reconstruction_out_feature: 1200  # the number of reconstruction feature, it is equal to in_feature
  supervised_out_feature: 0         # the number of cell type   # TODO: should be determined in dataset
  n_blocks: 3                       # the number of the attention block
  attention_n_heads: 8              # the number of the attention head
  attention_dropout: 0.2            # the dropout rate of attention
  ffn_d_hidden: 192                 # the dimension of feedforward
  ffn_dropout: 0.1                  # the dropout rate of feedforward
  residual_dropout: 0.0             # the dropout rate of residual
  cls: True                         # whether append cls
  pre_normalization: True           # whether normalization before residual
  global_token: True                # whether use global token

Finetune:
  pretrained_backbone: "FlashAttention"           # "FastFormer" or "FlashAttention"
  save_folder: './finetune_out'
  objective: 'both'   # Options: [reconstruction, contrastive, both]  

  method: 'heavy'     # Options: [light, heavy:n]. For heavy:n means an early stop patience of n epochs
  light_epochs: 25    # maximum epoch for light finetune
  max_epochs: 5000    # maximum epoch for heavy finetune
  patience: 100        # early stop patience for heavy finetune
  learning_rate: 0.0001
  weight_decay: 0.95
  gradient_clip_val: 0.5  # default algorithm is clip_grad_norm
  enable_batch: False

  explicit_zero_prob: False  # the probability of the expression being zero, approaching 0 or 1
  do_mgm: False  # loss for masked gene modeling
  do_cmgm: False  # loss for cell context-aware masked gene modeling
  cmgm_decoder_style: 'inner product'  # style of mvc decoder, choice from "inner product", "concat query", "sum query"
  mask_rate: 0.4  # the rate of masking for mlm and mvc
  mask_value: -1  # the value of masking for mlm and mvc
  do_dab: False  # employ a specific MLP classifier for batch correction
  dab_weight: 1.0  # weight for dab loss
  do_rcs: False  # reconstruction loss align to the pre-training stage
  embed_style: 'cls'  # the style of get cell embedding, choice from "cls", "avg-pool", "w-pool"
  augmentation_mode: 'corruption'
  corruption_rate: 0.6
  temperature: 0.07

