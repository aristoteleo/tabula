{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Tabula for Multi-omics & Multi-batch Integration\n",
    "\n",
    "In this tutorial, we illustrate the finetuning steps for the downstream task multi-omics and multi-batch integration.\n",
    "\n",
    "Here we takes DC dataset which contains scRNA-seq data from human blood dendritic cells (DCs), it contains two batches, each with four distinct cell types. Please refer to our manuscript for more information regarding the dataset. The processed dataset can be downloaded from the following link: https://drive.google.com/drive/folders/12Wg6fUe2MG8UBpMVi6SZKsFUZo4ai-UR?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from tabula.finetune.tokenizer import GeneVocab\n",
    "from scipy.sparse import issparse\n",
    "from tabula import logger\n",
    "from tabula.finetune.dataloader import MultiOmicsDataset\n",
    "from tabula.finetune.setup.integration import MultiOmicsIntegration\n",
    "from tabula.finetune.preprocessor import Preprocessor, get_pretrained_model\n",
    "from tabula.finetune.utils import FinetuneConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from anndata import AnnData\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-define parameters\n",
    "- For detailed finetuning parameters, please refer to and modify the yaml file in params['config_path']\n",
    "- For model weight, please download from this link: https://drive.google.com/drive/folders/19uG3hmvBZr2Zr4mWgIU-8SQ1dSg8GZuJ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seed': 23,\n",
    "    'config_path': '../resource/finetune_framework_integration.yaml',\n",
    "    'save_folder': 'finetune_out/integration_dc_test',\n",
    "    'model_path': '../weight/brain.pth',\n",
    "    'device': 'cuda:0',  # 'cuda:0' or 'cpu'\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    'data_path': '../data/integration/DC_raw.h5ad',\n",
    "    'vocab_path': '../resource/vocab.json',\n",
    "    'n_bins': 51,\n",
    "    'n_hvg': 2000,\n",
    "    'data_is_raw': True,\n",
    "    'batch_size': 32,\n",
    "    'n_workers': 4,\n",
    "}\n",
    "\n",
    "if_wandb = True\n",
    "wandb_params = {\n",
    "    'key': '644b123473f38af040ef215020d8e45acdf48fda',\n",
    "    'project': 'Integration_tutorial_test',\n",
    "    'entity': 'sctab-downstream',\n",
    "    'task': 'integration_dc_test'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(params['seed'])\n",
    "os.makedirs(params['save_folder'], exist_ok=True)\n",
    "finetune_config = FinetuneConfig(seed=params['seed'], config_path=params['config_path'])\n",
    "\n",
    "finetune_config.set_finetune_param('enable_wandb', if_wandb)\n",
    "\n",
    "finetune_config.set_finetune_param('save_folder', params['save_folder'])\n",
    "logger.info(f'Configuration loaded from {params[\"config_path\"]}, save finetuning result to {params[\"save_folder\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_wandb:\n",
    "    wandb.login(key=wandb_params['key'])\n",
    "    wandb.init(project=wandb_params['project'], entity=wandb_params['entity'], name=wandb_params['task'])\n",
    "    wandb_logger = WandbLogger(project=wandb_params['project'], log_model=False, offline=False)\n",
    "    logger.info(f'Wandb logging enabled')\n",
    "else:\n",
    "    wandb_logger = None\n",
    "    logger.info(f'Wandb logging disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(data_params['data_path'])\n",
    "ori_batch_col = \"batch\"\n",
    "adata.obs[\"celltype\"] = adata.obs[\"celltype\"].astype(\"category\")\n",
    "\n",
    "# make the batch category column\n",
    "adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "logger.info(f'Number of batches: {len(set(adata.obs[\"str_batch\"]))}')\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "# retain the common gene set between the data and the pre-trained model for further fine-tuning\n",
    "vocab = GeneVocab.from_file(data_params['vocab_path'])\n",
    "adata.var[\"id_in_vocab\"] = [\n",
    "    1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]]\n",
    "gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "logger.info(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\")\n",
    "adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_params['data_is_raw'],  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=data_params['n_hvg'],  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_params['data_is_raw'] else \"cell_ranger\",\n",
    "    binning=data_params['n_bins'],  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"str_batch\")\n",
    "\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltype_id_labels = adata.obs['celltype'].cat.codes.values\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "if num_batch_types > 1:\n",
    "    finetune_config.set_finetune_param('enable_batch', True)\n",
    "    finetune_config.set_finetune_param('n_batch', num_batch_types)\n",
    "else:\n",
    "    finetune_config.set_finetune_param('enable_batch', False)\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "dataset = MultiOmicsDataset(\n",
    "    expression_table=all_counts,\n",
    "    gene_ids=genes,\n",
    "    labels=celltype_id_labels,\n",
    "    batch_id=batch_ids,\n",
    "    vocab_file=data_params['vocab_path'],\n",
    "    in_feature=finetune_config.get_model_param('in_feature')\n",
    ")\n",
    "\n",
    "# split train and valid\n",
    "train_idx, valid_idx = train_test_split(np.arange(len(dataset)), test_size=0.1, random_state=params['seed'])\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "valid_dataset = torch.utils.data.Subset(dataset, valid_idx)\n",
    "train_loader = DataLoader(train_dataset, num_workers=data_params['n_workers'], shuffle=True, batch_size=data_params['batch_size'], drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, num_workers=data_params['n_workers'], shuffle=False, batch_size=data_params['batch_size'], drop_last=False)\n",
    "logger.info(f'Finish building train and valid loader, train size: {len(train_loader.dataset)}, valid size: {len(valid_loader.dataset)}')\n",
    "\n",
    "# construct a adata_eval for evaluation based on all data\n",
    "adata_eval = AnnData(\n",
    "    X=adata.layers[\"X_binned\"],\n",
    "    obs=adata.obs,\n",
    "    var=pd.DataFrame(index=adata.var[\"gene_name\"].tolist()),\n",
    "    layers={\"X_binned\": adata.layers[\"X_binned\"], }\n",
    ")\n",
    "# add one layer for tokenized vocab of genes\n",
    "adata_eval.layers[\"X_vocab\"] = np.zeros_like(adata_eval.layers[\"X_binned\"])\n",
    "vocab_lookup_table = dataset.vocab\n",
    "for i, gene in enumerate(adata_eval.var.index.tolist()):\n",
    "    if gene in vocab_lookup_table:\n",
    "        adata_eval.layers[\"X_vocab\"][:, i] = vocab_lookup_table[gene]\n",
    "adata_eval.var[\"gene_name\"] = adata_eval.var.index.tolist()\n",
    "adata_eval.obs[\"batch\"] = adata.obs[\"batch_id\"].tolist()\n",
    "# assign str_batch\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_batch = le.fit_transform(adata_eval.obs['batch'].values)  # batches: 0, 1, 2\n",
    "adata_eval.obs[\"batch_id\"] = encoded_batch\n",
    "adata_eval.obs[\"str_batch\"] = adata_eval.obs[\"batch_id\"].astype('category')\n",
    "\n",
    "test_dataset = MultiOmicsDataset(expression_table=adata_eval.layers[\"X_binned\"],\n",
    "                                 gene_ids=adata_eval.var[\"gene_name\"],\n",
    "                                 labels=adata_eval.obs['celltype'].cat.codes.values,\n",
    "                                 batch_id=adata_eval.obs['batch_id'].values,\n",
    "                                 vocab_file=data_params['vocab_path'],\n",
    "                                 in_feature=finetune_config.get_model_param('in_feature')\n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=data_params['batch_size'], shuffle=False, num_workers=data_params['n_workers'], drop_last=False)\n",
    "logger.info(f'Finish building test loader, test size: {len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['device'] != 'cpu' and not torch.cuda.is_available():\n",
    "    logger.error(f'Cuda is not available, change device to cpu')\n",
    "    params['device'] = 'cpu'\n",
    "tabula_pl_model = get_pretrained_model(\n",
    "    finetune_config=finetune_config,\n",
    "    model_path=params['model_path'],\n",
    "    device=params['device']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_trainer = MultiOmicsIntegration(\n",
    "    config=finetune_config,\n",
    "    tabula_model=tabula_pl_model,\n",
    "    wandb_logger=wandb_logger,\n",
    "    device=params['device'],\n",
    "    batch_size=data_params['batch_size'],\n",
    "    gene_ids=adata_eval.var[\"gene_name\"],\n",
    "    eval_adata=adata_eval,\n",
    "    dataloaders={'train_loader': train_loader, \n",
    "                 'val_loader': valid_loader,\n",
    "                 'test_loader': test_loader}\n",
    "    )\n",
    "\n",
    "integration_trainer.finetune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sctabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
